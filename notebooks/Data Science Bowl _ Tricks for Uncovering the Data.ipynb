{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided with the data for game analytics for the **PBS KIDS Measure Up!** app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: \n",
    "                    * Bird Measurer\n",
    "                    * Cart Balancer\n",
    "                    * Cauldron Filler\n",
    "                    * Chest Sorter\n",
    "                    * Mshroom Sorter.\n",
    "\n",
    "The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About this kernel**\n",
    "\n",
    "  This kernel acts as a starter kit. It gives all the essential Key insights on the data as well as modelling\n",
    "  \n",
    "**Key Takeaways**\n",
    "\n",
    "* Extensive EDA\n",
    "* Effective Story Telling\n",
    "* Creative Feature Engineering\n",
    "* Modelling\n",
    "* Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of the plots made here are interactive please feel free to hover over**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the necessary Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from plotly.offline import iplot\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Reading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/mnt/7917b992-0701-4012-95c1-8b06e49e8f9b/2019DataScienceBowl/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path+'train.csv')\n",
    "train_labels = pd.read_csv(path+'train_labels.csv')\n",
    "test_df = pd.read_csv(path+'test.csv')\n",
    "specs_Df = pd.read_csv(path+'specs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11341042, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_id           0\n",
       "game_session       0\n",
       "timestamp          0\n",
       "event_data         0\n",
       "installation_id    0\n",
       "event_count        0\n",
       "event_code         0\n",
       "game_time          0\n",
       "title              0\n",
       "type               0\n",
       "world              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This file demonstrates how to compute the ground truth for the assessments in the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_session</th>\n",
       "      <th>installation_id</th>\n",
       "      <th>title</th>\n",
       "      <th>num_correct</th>\n",
       "      <th>num_incorrect</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6bdf9623adc94d89</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>Mushroom Sorter (Assessment)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>77b8ee947eb84b4e</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>Bird Measurer (Assessment)</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>901acc108f55a5a1</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>Mushroom Sorter (Assessment)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9501794defd84e4d</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>Mushroom Sorter (Assessment)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>a9ef3ecb3d1acc6a</td>\n",
       "      <td>0006a69f</td>\n",
       "      <td>Bird Measurer (Assessment)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       game_session installation_id                         title  \\\n",
       "0  6bdf9623adc94d89        0006a69f  Mushroom Sorter (Assessment)   \n",
       "1  77b8ee947eb84b4e        0006a69f    Bird Measurer (Assessment)   \n",
       "2  901acc108f55a5a1        0006a69f  Mushroom Sorter (Assessment)   \n",
       "3  9501794defd84e4d        0006a69f  Mushroom Sorter (Assessment)   \n",
       "4  a9ef3ecb3d1acc6a        0006a69f    Bird Measurer (Assessment)   \n",
       "\n",
       "   num_correct  num_incorrect  accuracy  accuracy_group  \n",
       "0            1              0       1.0               3  \n",
       "1            0             11       0.0               0  \n",
       "2            1              0       1.0               3  \n",
       "3            1              1       0.5               2  \n",
       "4            1              0       1.0               3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I have mentioned, there are five assesments let's explore the ground truth of each assesment(sucess and failure rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
       "       'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)',\n",
       "       'Cart Balancer (Assessment)'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.title.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the ground truth of each assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may wonder what does the **accuracy_group** denotes. No worries as mentioned in the [data description](https://www.kaggle.com/c/data-science-bowl-2019/data), They denote:\n",
    "\n",
    "**3**: the assessment was solved on the first attempt\n",
    "\n",
    "**2**: the assessment was solved on the second attempt\n",
    "\n",
    "**1**: the assessment was solved after 3 or more attempts\n",
    "\n",
    "**0**: the assessment was never solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of the Accuracy group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "Success_Rate_1=pd.DataFrame()\n",
    "Success_Rate_2=pd.DataFrame()\n",
    "Success_Rate_3=pd.DataFrame()\n",
    "Success_Rate_4=pd.DataFrame()\n",
    "Success_Rate_5=pd.DataFrame()\n",
    "Mushroom_Sorter=train_labels.loc[train_labels['title'] == 'Mushroom Sorter (Assessment)']\n",
    "Success_Rate_1['Type']=Mushroom_Sorter.num_correct.value_counts().index\n",
    "Success_Rate_1['Count']=Mushroom_Sorter.num_correct.value_counts().values\n",
    "Bird_Measurer=train_labels.loc[train_labels['title'] ==  'Bird Measurer (Assessment)']\n",
    "Success_Rate_2['Type']=Bird_Measurer.num_correct.value_counts().index\n",
    "Success_Rate_2['Count']=Bird_Measurer.num_correct.value_counts().values\n",
    "Cauldron_Filler=train_labels.loc[train_labels['title'] == 'Cauldron Filler (Assessment)']\n",
    "Success_Rate_3['Type']=Cauldron_Filler.num_correct.value_counts().index\n",
    "Success_Rate_3['Count']=Cauldron_Filler.num_correct.value_counts().values\n",
    "Chest_Sorter=train_labels.loc[train_labels['title'] == 'Chest Sorter (Assessment)']\n",
    "Success_Rate_4['Type']=Chest_Sorter.num_correct.value_counts().index\n",
    "Success_Rate_4['Count']=Chest_Sorter.num_correct.value_counts().values\n",
    "Cart_Balancer=train_labels.loc[train_labels['title'] == 'Cart Balancer (Assessment)']\n",
    "Success_Rate_5['Type']=Cart_Balancer.num_correct.value_counts().index\n",
    "Success_Rate_5['Count']=Cart_Balancer.num_correct.value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "labels = [0,1]\n",
    "\n",
    "fig = make_subplots(3, 2, specs=[[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}]],\n",
    "                    subplot_titles=['Mushroom Sorter', 'Bird Measurer','Cauldron Filler','Chest Sorter','Cart Balancer'])\n",
    "fig.add_trace(go.Pie(labels=Success_Rate_1['Type'], values=Success_Rate_1['Count'], scalegroup='one',\n",
    "                     name=\"Success Rate\"), 1, 1)\n",
    "fig.add_trace(go.Pie(labels=Success_Rate_2['Type'], values=Success_Rate_2['Count'], scalegroup='one',\n",
    "                     name=\"Success Rate\"), 1, 2)\n",
    "fig.add_trace(go.Pie(labels=Success_Rate_3['Type'], values=Success_Rate_3['Count'], scalegroup='one',\n",
    "                     name=\"Success Rate\"), 2, 1)\n",
    "fig.add_trace(go.Pie(labels=Success_Rate_4['Type'], values=Success_Rate_4['Count'], scalegroup='one',\n",
    "                     name=\"Success Rate\"), 2, 2)\n",
    "fig.add_trace(go.Pie(labels=Success_Rate_5['Type'], values=Success_Rate_5['Count'], scalegroup='one',\n",
    "                     name=\"Success Rate\"), 3, 1)\n",
    "\n",
    "fig.update_layout(title_text='Success Rate of Each Group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Data Minification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the dataset is pretty big, so we are trying to make the dataset smaller without losing information.\n",
    "\n",
    "Reason behind memory Reduction:\n",
    "\n",
    "Int16: 2 bytes\n",
    "\n",
    "Int32 and int: 4 bytes\n",
    "\n",
    "Int64 : 8 bytes\n",
    "\n",
    "This is an example how different integer types are occupying the memory. In many cases it is not necessary to represent our integer as int64 and int32 it is just waste of memory. So I am trying to understand the necessaity of every numerical representation and try to convert the unnecessary higher numerical representation to lower one. In that, we can reduce the memory without losing the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            \n",
    "            #Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "               NAlist.append(col)\n",
    "               df[col].fillna(-999,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] =df[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df,NAlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing Memory For training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,train_Na=reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing Memory For test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df,test_Na=reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the Train data to the Fullest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Unique_event':[train_df.event_id.nunique()],\n",
    "      'Unique_gamesession':[train_df.game_session.nunique()],\n",
    "      'Unique_title':[train_df.title.nunique()]}\n",
    "Count_df=pd.DataFrame(data)\n",
    "Count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Figure's**\n",
    "\n",
    "Of total **113441042** records in train data there are, only\n",
    "\n",
    "1. **384** Unique events happened\n",
    "2. **303319** Unique Gamming Session\n",
    "3. **44** Uninque Game Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timestamp**\n",
    "\n",
    "Let's explore the time stamp of the given data and try to find some useful information\n",
    "\n",
    "This section was taken from ths amazing [kernel](https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction). I just added a how the test and train data are in given timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and make date / hour features\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "train_df['date'] = train_df['timestamp'].dt.date\n",
    "train_df['hour'] = train_df['timestamp'].dt.hour\n",
    "train_df['weekday_name'] = train_df['timestamp'].dt.weekday_name\n",
    "\n",
    "# Same for test\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "test_df['date'] = test_df['timestamp'].dt.date\n",
    "test_df['hour'] = test_df['timestamp'].dt.hour\n",
    "test_df['weekday_name'] = test_df['timestamp'].dt.weekday_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_df.groupby('date')['event_id'].agg('count').plot(figsize=(15, 3),title='Numer of Event Observations by Date',\n",
    "                                                       color=\"blue\")\n",
    "test_df.groupby('date')['event_id'].agg('count').plot(figsize=(15, 3),title='Numer of Event Observations by Date'\n",
    "                                                      ,color=\"yellow\")\n",
    "train_patch = mpatches.Patch(color='blue', label='Train data')\n",
    "test_patch = mpatches.Patch(color='yellow', label='Test data')\n",
    "plt.legend(handles=[train_patch, test_patch])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('hour')['event_id'].agg('count').plot(figsize=(15, 3),title='Numer of Event Observations by Hour',color=\"blue\")\n",
    "test_df.groupby('hour')['event_id'].agg('count').plot(figsize=(15, 3),title='Numer of Event Observations by Hour',color=\"yellow\")\n",
    "train_patch = mpatches.Patch(color='blue', label='Train data')\n",
    "test_patch = mpatches.Patch(color='yellow', label='Test data')\n",
    "plt.legend(handles=[train_patch, test_patch])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('weekday_name')['event_id'].agg('count').T[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']].T.plot(figsize=(15, 3),title='Numer of Event Observations by Day of Week',color=\"blue\")\n",
    "test_df.groupby('weekday_name')['event_id'].agg('count').T[['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']].T.plot(figsize=(15, 3),title='Numer of Event Observations by Day of Week',color=\"yellow\")\n",
    "train_patch = mpatches.Patch(color='blue', label='Train data')\n",
    "test_patch = mpatches.Patch(color='yellow', label='Test data')\n",
    "plt.legend(handles=[train_patch, test_patch])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribtuion of Game Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game=pd.DataFrame()\n",
    "Game['Type']=train_df.type.value_counts().index\n",
    "Game['Count']=train_df.type.value_counts().values\n",
    "\n",
    "import plotly.offline as pyo\n",
    "py.init_notebook_mode(connected=True)\n",
    "fig = go.Figure(data=[go.Pie(labels=Game['Type'], values=Game['Count'],hole=0.2)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Game Title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game=pd.DataFrame()\n",
    "Game['Title']=train_df.title.value_counts().index\n",
    "Game['Count']=train_df.title.value_counts().values\n",
    "\n",
    "fig = px.bar(Game, x='Title', y='Count',\n",
    "             hover_data=['Count'], color='Count',\n",
    "             labels={'pop':'Total Number of game titles'}, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Game Type VS Game Played Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time=[]\n",
    "type_=[]\n",
    "for i in train_df.type.unique():\n",
    "    type_.append(i)\n",
    "    avg_time.append(train_df.loc[train_df['type'] ==i]['game_time'].mean())\n",
    "    \n",
    "Avg_Timeplayed=pd.DataFrame()\n",
    "Avg_Timeplayed['Type']=type_\n",
    "Avg_Timeplayed['Average']=avg_time\n",
    "\n",
    "fig = px.bar(Avg_Timeplayed, x='Type', y='Average',\n",
    "             hover_data=['Average'], color='Average',\n",
    "             labels={'pop':'Average time played on each types'}, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Game Title VS Game Played Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time=[]\n",
    "title_=[]\n",
    "for i in train_df.title.unique():\n",
    "    title_.append(i)\n",
    "    avg_time.append(train_df.loc[train_df['title'] ==i]['game_time'].mean())\n",
    "    \n",
    "Avg_Timeplayed=pd.DataFrame()\n",
    "Avg_Timeplayed['Title']=title_\n",
    "Avg_Timeplayed['Average']=avg_time\n",
    "\n",
    "fig = px.bar(Avg_Timeplayed, x='Title', y='Average',\n",
    "             hover_data=['Average'], color='Average',\n",
    "             labels={'pop':'Average time played on each titles'}, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This kernel is under construction, Stay tuned for more updates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please upvote if you find this kernel useful**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
